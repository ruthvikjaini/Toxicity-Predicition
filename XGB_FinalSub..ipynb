{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(os.getcwd(), 'train.csv')\n",
    "feature_path = os.path.join(os.getcwd(), 'feamat.csv')\n",
    "test_path = os.path.join(os.getcwd(), 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_path)\n",
    "feature_data = pd.read_csv(feature_path)\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "traget_data = train_data[\"Expected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[['chemical_id','assay_id']] = train_data.Id.str.split(\";\",expand=True) \n",
    "test_data[['chemical_id','assay_id']] = test_data.x.str.split(\";\",expand=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data.replace([np.inf, -np.inf], np.nan, inplace =True)\n",
    "feature_data['V15'].fillna(value=feature_data['V15'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V1066</th>\n",
       "      <th>V1067</th>\n",
       "      <th>V1068</th>\n",
       "      <th>V1069</th>\n",
       "      <th>V1070</th>\n",
       "      <th>V1071</th>\n",
       "      <th>V1072</th>\n",
       "      <th>V1073</th>\n",
       "      <th>V1074</th>\n",
       "      <th>V1075</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60-35-5</td>\n",
       "      <td>178</td>\n",
       "      <td>59.037114</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>43.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103-90-2</td>\n",
       "      <td>1983</td>\n",
       "      <td>151.063329</td>\n",
       "      <td>0.870</td>\n",
       "      <td>49.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.142259</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>968-81-0</td>\n",
       "      <td>1989</td>\n",
       "      <td>324.114378</td>\n",
       "      <td>2.960</td>\n",
       "      <td>100.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185395</td>\n",
       "      <td>0.161948</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>520-45-6</td>\n",
       "      <td>122903</td>\n",
       "      <td>168.042259</td>\n",
       "      <td>-0.551</td>\n",
       "      <td>60.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.198742</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50594-66-6</td>\n",
       "      <td>44073</td>\n",
       "      <td>360.996485</td>\n",
       "      <td>4.557</td>\n",
       "      <td>89.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.276855</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1075 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           V1      V2          V3     V4      V5   V6   V7   V8        V9  \\\n",
       "0     60-35-5     178   59.037114 -0.808   43.09  0.0  0.0  0.0  0.000000   \n",
       "1    103-90-2    1983  151.063329  0.870   49.33  0.0  0.0  0.0  0.083333   \n",
       "2    968-81-0    1989  324.114378  2.960  100.72  0.0  0.0  0.0  0.185395   \n",
       "3    520-45-6  122903  168.042259 -0.551   60.44  0.0  0.0  0.0  0.055556   \n",
       "4  50594-66-6   44073  360.996485  4.557   89.67  0.0  0.0  0.0  0.136083   \n",
       "\n",
       "        V10  ...  V1066  V1067  V1068  V1069  V1070  V1071  V1072  V1073  \\\n",
       "0  0.000000  ...      0      0      0      0      0      0      0      0   \n",
       "1  0.142259  ...      0      0      0      0      0      0      0      0   \n",
       "2  0.161948  ...      0      0      0      0      0      0      0      0   \n",
       "3  0.198742  ...      0      0      0      0      0      0      0      0   \n",
       "4  0.276855  ...      0      0      0      0      0      0      0      0   \n",
       "\n",
       "   V1074  V1075  \n",
       "0      0      0  \n",
       "1      0      0  \n",
       "2      0      0  \n",
       "3      0      0  \n",
       "4      0      0  \n",
       "\n",
       "[5 rows x 1075 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_matrix():\n",
    "    \"\"\"\n",
    "    get feature matrix data\n",
    "    :return: feature matrix\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(train_path)\n",
    "    label_data = data[\"Expected\"].values\n",
    "    Id_list = data['Id'].values\n",
    "    feature_matrix = []\n",
    "    for id in Id_list:\n",
    "        chemical_id, assay_id = id.split(';')\n",
    "        matched_data = feature_data.loc[feature_data['V1'] == chemical_id].values[0]\n",
    "        matched_data[1] = int(assay_id)\n",
    "        feature_matrix.append(matched_data)\n",
    "    return np.array(label_data), np.array(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17122401, 0.59273088, 0.05200648, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.0755261 , 0.57114521, 0.02194144, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.19838607, 0.74791436, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.08201345, 0.52047722, 0.08923109, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.05874252, 0.55084301, 0.02194144, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.36312399, 0.74027186, 0.1268156 , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data, raw_feature_matrix = get_feature_matrix()\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "feature_matrix = min_max_scaler.fit_transform(raw_feature_matrix[:, 2:])\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(feature_matrix, dtype=float)\n",
    "Y= label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_features(X, Y, maximum_feature_num=1000):\n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=1000)\n",
    "    model = bestfeatures.fit(np.abs(X),Y)\n",
    "    dfscores = pd.Series(model.scores_)\n",
    "    dfscores.nlargest(maximum_feature_num).plot(kind='bar')\n",
    "    plt.show()\n",
    "    feature_columns = dfscores.nlargest(maximum_feature_num)\n",
    "    feature_columns.index\n",
    "    X_features = X[:, feature_columns.index]\n",
    "    return feature_columns.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([ 378,  969,  889,  888,  906,  204,  384,  449,  782,  385,\n",
      "            ...\n",
      "            1001,  683,  408,  592,   18,  556,  277, 1058,  789,  560],\n",
      "           dtype='int64', length=882)\n"
     ]
    }
   ],
   "source": [
    "top_feature_columns = best_features(X, Y)\n",
    "print(top_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n",
       "         11,   12,   13,   14,   15,   16,   17,   18,   19,   20,   21,\n",
       "         22,   23,   24,   25,   28,   30,   31,   32,   33,   34,   35,\n",
       "         36,   37,   38,   39,   40,   41,   42,   43,   44,   45,   46,\n",
       "         47,   48,   49,   50,   51,   52,   53,   54,   55,   56,   57,\n",
       "         58,   59,   60,   61,   62,   63,   64,   65,   66,   67,   68,\n",
       "         69,   70,   71,   72,   73,   74,   75,   76,   77,   78,   79,\n",
       "         80,   81,   82,   83,   84,   85,   86,   87,   88,   89,   90,\n",
       "         91,   92,   93,   94,   95,   96,   97,   98,   99,  100,  101,\n",
       "        102,  103,  104,  105,  106,  107,  108,  109,  110,  111,  112,\n",
       "        113,  114,  115,  116,  117,  118,  119,  120,  121,  122,  123,\n",
       "        124,  125,  126,  127,  128,  129,  130,  131,  132,  133,  134,\n",
       "        135,  136,  137,  138,  139,  140,  141,  142,  143,  144,  145,\n",
       "        146,  147,  148,  149,  150,  151,  152,  153,  154,  155,  156,\n",
       "        157,  158,  159,  160,  161,  162,  163,  164,  165,  166,  167,\n",
       "        168,  169,  170,  171,  172,  173,  174,  175,  176,  177,  178,\n",
       "        179,  180,  181,  182,  183,  184,  185,  186,  187,  188,  189,\n",
       "        190,  191,  192,  193,  194,  195,  198,  201,  202,  203,  204,\n",
       "        205,  206,  207,  208,  209,  210,  211,  212,  213,  214,  215,\n",
       "        216,  217,  218,  219,  220,  221,  222,  223,  225,  226,  227,\n",
       "        228,  229,  230,  231,  232,  233,  235,  236,  237,  238,  239,\n",
       "        240,  242,  243,  244,  245,  246,  248,  249,  250,  251,  252,\n",
       "        253,  254,  257,  268,  270,  271,  272,  273,  277,  287,  294,\n",
       "        297,  300,  304,  307,  308,  309,  310,  314,  316,  317,  321,\n",
       "        322,  323,  324,  328,  329,  335,  336,  337,  338,  339,  340,\n",
       "        341,  342,  343,  344,  345,  346,  347,  348,  349,  350,  351,\n",
       "        352,  354,  355,  356,  363,  370,  371,  372,  373,  374,  375,\n",
       "        376,  377,  378,  379,  380,  381,  382,  383,  384,  385,  386,\n",
       "        387,  388,  391,  392,  393,  394,  398,  399,  401,  405,  406,\n",
       "        407,  408,  409,  410,  411,  412,  416,  417,  418,  419,  421,\n",
       "        422,  433,  434,  437,  440,  443,  447,  448,  449,  450,  451,\n",
       "        452,  453,  466,  468,  469,  475,  476,  477,  478,  479,  483,\n",
       "        484,  485,  486,  487,  489,  490,  491,  492,  493,  495,  496,\n",
       "        497,  498,  499,  500,  501,  504,  505,  506,  519,  520,  521,\n",
       "        522,  523,  524,  525,  526,  527,  528,  529,  530,  531,  532,\n",
       "        533,  534,  535,  536,  537,  538,  539,  540,  541,  542,  543,\n",
       "        544,  545,  546,  547,  548,  549,  550,  551,  552,  553,  554,\n",
       "        555,  556,  557,  558,  559,  560,  561,  562,  563,  564,  565,\n",
       "        566,  567,  568,  569,  570,  571,  572,  573,  574,  575,  576,\n",
       "        577,  578,  579,  580,  581,  582,  583,  584,  585,  586,  587,\n",
       "        588,  589,  590,  591,  592,  593,  595,  596,  597,  598,  599,\n",
       "        600,  601,  602,  603,  604,  605,  606,  607,  608,  609,  610,\n",
       "        611,  612,  613,  614,  615,  617,  619,  620,  621,  622,  623,\n",
       "        624,  625,  626,  627,  628,  629,  630,  631,  632,  633,  634,\n",
       "        635,  636,  637,  638,  639,  640,  641,  642,  643,  644,  645,\n",
       "        646,  647,  648,  649,  650,  651,  652,  653,  654,  656,  657,\n",
       "        658,  659,  660,  661,  662,  663,  664,  665,  666,  667,  668,\n",
       "        669,  670,  671,  672,  673,  674,  675,  676,  677,  678,  679,\n",
       "        680,  681,  682,  683,  684,  685,  686,  687,  688,  689,  690,\n",
       "        691,  692,  693,  694,  695,  696,  697,  698,  699,  700,  701,\n",
       "        702,  703,  704,  705,  706,  707,  708,  709,  710,  711,  712,\n",
       "        713,  714,  715,  716,  717,  718,  719,  720,  721,  722,  723,\n",
       "        724,  725,  726,  727,  728,  729,  730,  731,  732,  733,  734,\n",
       "        735,  736,  737,  738,  739,  740,  741,  742,  743,  744,  745,\n",
       "        746,  747,  748,  749,  750,  751,  752,  753,  754,  755,  756,\n",
       "        757,  758,  759,  760,  761,  762,  763,  764,  765,  766,  767,\n",
       "        768,  769,  770,  771,  772,  773,  774,  775,  776,  777,  778,\n",
       "        779,  780,  781,  782,  783,  784,  785,  786,  787,  788,  789,\n",
       "        790,  791,  792,  793,  794,  795,  796,  797,  798,  799,  800,\n",
       "        801,  802,  803,  804,  805,  806,  807,  808,  809,  810,  811,\n",
       "        812,  813,  814,  815,  816,  817,  818,  819,  820,  821,  822,\n",
       "        823,  824,  825,  826,  827,  828,  829,  830,  831,  832,  833,\n",
       "        834,  835,  836,  837,  838,  839,  840,  841,  842,  843,  844,\n",
       "        845,  846,  847,  848,  849,  850,  851,  852,  853,  854,  855,\n",
       "        856,  857,  858,  859,  860,  861,  862,  863,  864,  865,  866,\n",
       "        867,  868,  869,  870,  871,  872,  873,  874,  875,  876,  877,\n",
       "        878,  879,  880,  881,  882,  883,  884,  885,  886,  887,  888,\n",
       "        889,  890,  891,  892,  893,  894,  895,  896,  897,  898,  899,\n",
       "        900,  901,  902,  903,  904,  905,  906,  907,  908,  909,  910,\n",
       "        911,  912,  913,  914,  915,  917,  918,  920,  921,  923,  925,\n",
       "        926,  927,  928,  929,  930,  931,  932,  933,  934,  935,  936,\n",
       "        937,  938,  939,  941,  942,  943,  944,  945,  946,  947,  948,\n",
       "        949,  950,  951,  952,  953,  954,  955,  956,  957,  959,  960,\n",
       "        962,  963,  964,  965,  966,  967,  968,  969,  970,  971,  972,\n",
       "        973,  974,  975,  976,  977,  978,  980,  981,  983,  984,  986,\n",
       "        988,  989,  990,  991,  992,  993,  994,  995,  996,  997,  998,\n",
       "        999, 1000, 1001, 1002, 1004, 1005, 1006, 1007, 1008, 1009, 1010,\n",
       "       1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1022,\n",
       "       1023, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034,\n",
       "       1035, 1040, 1049, 1052, 1053, 1055, 1056, 1058, 1059, 1060, 1061,\n",
       "       1064, 1070], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(top_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_columns = ['V26', 'V27', 'V29', 'V196', 'V197', 'V199', 'V200', 'V224', 'V234', 'V241', 'V247', 'V255', 'V256',\n",
    "                  'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V269', 'V274', 'V275',\n",
    "                  'V276', 'V278', 'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286', 'V288', 'V289', 'V290',\n",
    "                  'V291', 'V292', 'V293', 'V295', 'V296', 'V298', 'V299', 'V301', 'V302', 'V303', 'V305', 'V306', 'V311', \n",
    "                  'V312', 'V313', 'V315', 'V318', 'V319', 'V320', 'V325', 'V326', 'V327', 'V330', 'V331', 'V332', 'V333', \n",
    "                  'V334', 'V353', 'V357', 'V358', 'V359', 'V360', 'V361', 'V362', 'V364', 'V365', 'V366', 'V367', 'V368', \n",
    "                  'V369', 'V389', 'V390', 'V395', 'V396', 'V397', 'V400', 'V402', 'V403', 'V404', 'V413', 'V414', 'V415', \n",
    "                  'V420', 'V423', 'V424', 'V425', 'V426', 'V427', 'V428', 'V429', 'V430', 'V431', 'V432', 'V435', 'V436', \n",
    "                  'V438', 'V439', 'V441', 'V442', 'V444', 'V445', 'V446', 'V454', 'V455', 'V456', 'V457', 'V458', 'V459',\n",
    "                  'V460', 'V461', 'V462', 'V463', 'V464', 'V465', 'V467', 'V470', 'V471', 'V472', 'V473', 'V474', 'V480', \n",
    "                  'V481', 'V482', 'V488', 'V494', 'V502', 'V503', 'V507', 'V508', 'V509', 'V510', 'V511', 'V512', 'V513',\n",
    "                  'V514', 'V515', 'V516', 'V517', 'V518', 'V594', 'V616', 'V618', 'V655', 'V916', 'V919', 'V922', 'V924', \n",
    "                  'V940', 'V958', 'V961', 'V979', 'V982', 'V985', 'V987', 'V1003', 'V1021', 'V1024', 'V1036', 'V1037', \n",
    "                  'V1038', 'V1039', 'V1041', 'V1042', 'V1043', 'V1044', 'V1045', 'V1046', 'V1047', 'V1048', 'V1050', \n",
    "                  'V1051', 'V1054', 'V1057', 'V1062', 'V1063', 'V1065', 'V1066', 'V1067', 'V1068', 'V1069']\n",
    "for column in remove_columns:\n",
    "     feature_data = feature_data.drop(column, axis = 1)\n",
    "for column in ['V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20',\n",
    "               'V21','V22','V23','V24','V25','V28','V30','V31','V32','V33','V34','V35','V36','V37','V38','V39','V40','V41',\n",
    "               'V42','V43','V44','V45','V46','V47','V48','V49','V50','V51','V52','V53','V54','V55','V56','V57','V58','V59',\n",
    "               'V60','V61','V62','V63','V64','V65','V66','V67','V68','V69','V70','V71','V72','V73','V74','V75','V76','V77',\n",
    "               'V78','V79','V80','V81','V82','V83','V84','V85','V86','V87','V88','V89','V90','V91','V92','V93','V94','V95',\n",
    "               'V96','V97','V98','V99','V100','V101','V102','V103','V104','V105','V106','V107','V108','V109','V110','V111',\n",
    "               'V112','V113','V114','V115','V116','V117','V118','V119','V120','V121','V122','V123','V124','V125','V126',\n",
    "               'V127','V128','V129','V130','V131','V132','V133','V134','V135','V136','V137','V138','V139','V140','V141',\n",
    "               'V142','V143','V144','V145','V146','V147','V148','V149','V150','V151','V152','V153','V154','V155','V156',\n",
    "               'V157','V158','V159','V160','V161','V162','V163','V164','V165','V166','V167','V168','V169','V170','V171',\n",
    "               'V172','V173','V174','V175','V176','V177','V178','V179','V180','V181','V182','V183','V184','V185','V186',\n",
    "               'V187','V188','V189','V190','V191','V192','V193','V194','V195','V198','V201','V202','V203','V204','V205',\n",
    "               'V206','V207','V208','V209','V210','V211','V212','V213','V214','V215','V216','V217','V218','V219','V220',\n",
    "               'V221','V222','V223','V225','V226','V227','V228','V229','V230','V231','V232','V233','V235','V236','V237',\n",
    "               'V238','V239','V240','V242','V243','V244','V245','V246','V248','V249','V250','V251','V252','V253','V254',\n",
    "               'V257','V268','V270','V271','V272','V273','V277','V287','V294','V297','V300','V304','V307','V308','V309',\n",
    "               'V310','V314','V316','V317','V321','V322','V323','V324','V328','V329','V335','V336','V337','V338','V339',\n",
    "               'V340','V341','V342','V343','V344','V345','V346','V347','V348','V349','V350','V351','V352','V354','V355',\n",
    "               'V356','V363','V370','V371','V372','V373','V374','V375','V376','V377','V378','V379','V380','V381','V382',\n",
    "               'V383','V384','V385','V386','V387','V388','V391','V392','V393','V394','V398','V399','V401','V405','V406',\n",
    "               'V407','V408','V409','V410','V411','V412','V416','V417','V418','V419','V421','V422','V433','V434','V437',\n",
    "               'V440','V443','V447','V448','V449','V450','V451','V452','V453','V466','V468','V469','V475','V476','V477',\n",
    "               'V478','V479','V483','V484','V485','V486','V487','V489','V490','V491','V492','V493','V495','V496','V497',\n",
    "               'V498','V499','V500','V501','V504','V505','V506','V519','V520','V521','V522','V523','V524','V525','V526',\n",
    "               'V527','V528','V529','V530','V531','V532','V533','V534','V535','V536','V537','V538','V539','V540','V541',\n",
    "               'V542','V543','V544','V545','V546','V547','V548','V549','V550','V551','V552','V553','V554','V555','V556',\n",
    "               'V557','V558','V559','V560','V561','V562','V563','V564','V565','V566','V567','V568','V569','V570','V571',\n",
    "               'V572','V573','V574','V575','V576','V577','V578','V579','V580','V581','V582','V583','V584','V585','V586',\n",
    "               'V587','V588','V589','V590','V591','V592','V593','V595','V596','V597','V598','V599','V600','V601','V602',\n",
    "               'V603','V604','V605','V606','V607','V608','V609','V610','V611','V612','V613','V614','V615','V617','V619',\n",
    "               'V620','V621','V622','V623','V624','V625','V626','V627','V628','V629','V630','V631','V632','V633','V634',\n",
    "               'V635','V636','V637','V638','V639','V640','V641','V642','V643','V644','V645','V646','V647','V648','V649',\n",
    "               'V650','V651','V652','V653','V654','V656','V657','V658','V659','V660','V661','V662','V663','V664','V665',\n",
    "               'V666','V667','V668','V669','V670','V671','V672','V673','V674','V675','V676','V677','V678','V679','V680',\n",
    "               'V681','V682','V683','V684','V685','V686','V687','V688','V689','V690','V691','V692','V693','V694','V695',\n",
    "               'V696','V697','V698','V699','V700','V701','V702','V703','V704','V705','V706','V707','V708','V709','V710',\n",
    "               'V711','V712','V713','V714','V715','V716','V717','V718','V719','V720','V721','V722','V723','V724','V725',\n",
    "               'V726','V727','V728','V729','V730','V731','V732','V733','V734','V735','V736','V737','V738','V739','V740',\n",
    "               'V741','V742','V743','V744','V745','V746','V747','V748','V749','V750','V751','V752','V753','V754','V755',\n",
    "               'V756','V757','V758','V759','V760','V761','V762','V763','V764','V765','V766','V767','V768','V769','V770',\n",
    "               'V771','V772','V773','V774','V775','V776','V777','V778','V779','V780','V781','V782','V783','V784','V785',\n",
    "               'V786','V787','V788','V789','V790','V791','V792','V793','V794','V795','V796','V797','V798','V799','V800',\n",
    "               'V801','V802','V803','V804','V805','V806','V807','V808','V809','V810','V811','V812','V813','V814','V815',\n",
    "               'V816','V817','V818','V819','V820','V821','V822','V823','V824','V825','V826','V827','V828','V829','V830',\n",
    "               'V831','V832','V833','V834','V835','V836','V837','V838','V839','V840','V841','V842','V843','V844','V845',\n",
    "               'V846','V847','V848','V849','V850','V851','V852','V853','V854','V855','V856','V857','V858','V859','V860',\n",
    "               'V861','V862','V863','V864','V865','V866','V867','V868','V869','V870','V871','V872','V873','V874','V875',\n",
    "               'V876','V877','V878','V879','V880','V881','V882','V883','V884','V885','V886','V887','V888','V889','V890',\n",
    "               'V891','V892','V893','V894','V895','V896','V897','V898','V899','V900','V901','V902','V903','V904','V905',\n",
    "               'V906','V907','V908','V909','V910','V911','V912','V913','V914','V915','V917','V918','V920','V921','V923',\n",
    "               'V925','V926','V927','V928','V929','V930','V931','V932','V933','V934','V935','V936','V937','V938','V939',\n",
    "               'V941','V942','V943','V944','V945','V946','V947','V948','V949','V950','V951','V952','V953','V954','V955',\n",
    "               'V956','V957','V959','V960','V962','V963','V964','V965','V966','V967','V968','V969','V970','V971','V972',\n",
    "               'V973','V974','V975','V976','V977','V978','V980','V981','V983','V984','V986','V988','V989','V990','V991',\n",
    "               'V992','V993','V994','V995','V996','V997','V998','V999','V1000','V1001','V1002','V1004','V1005','V1006',\n",
    "               'V1007','V1008','V1009','V1010','V1011','V1012','V1013','V1014','V1015','V1016','V1017','V1018','V1019',\n",
    "               'V1020','V1022','V1023','V1025','V1026','V1027','V1028','V1029','V1030','V1031','V1032','V1033','V1034',\n",
    "               'V1035','V1040','V1049','V1052','V1053','V1055','V1056','V1058','V1059','V1060','V1061','V1064','V1070']:\n",
    "     feature_data[column].replace(to_replace=0, value = feature_data[column].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V1059</th>\n",
       "      <th>V1060</th>\n",
       "      <th>V1061</th>\n",
       "      <th>V1064</th>\n",
       "      <th>V1070</th>\n",
       "      <th>V1071</th>\n",
       "      <th>V1072</th>\n",
       "      <th>V1073</th>\n",
       "      <th>V1074</th>\n",
       "      <th>V1075</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60-35-5</td>\n",
       "      <td>178</td>\n",
       "      <td>59.037114</td>\n",
       "      <td>-0.808</td>\n",
       "      <td>43.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102062</td>\n",
       "      <td>0.184195</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103-90-2</td>\n",
       "      <td>1983</td>\n",
       "      <td>151.063329</td>\n",
       "      <td>0.870</td>\n",
       "      <td>49.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.142259</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>968-81-0</td>\n",
       "      <td>1989</td>\n",
       "      <td>324.114378</td>\n",
       "      <td>2.960</td>\n",
       "      <td>100.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185395</td>\n",
       "      <td>0.161948</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>520-45-6</td>\n",
       "      <td>122903</td>\n",
       "      <td>168.042259</td>\n",
       "      <td>-0.551</td>\n",
       "      <td>60.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.198742</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50594-66-6</td>\n",
       "      <td>44073</td>\n",
       "      <td>360.996485</td>\n",
       "      <td>4.557</td>\n",
       "      <td>89.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136083</td>\n",
       "      <td>0.276855</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 886 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           V1      V2          V3     V4      V5   V6   V7   V8        V9  \\\n",
       "0     60-35-5     178   59.037114 -0.808   43.09  0.0  0.0  0.0  0.102062   \n",
       "1    103-90-2    1983  151.063329  0.870   49.33  0.0  0.0  0.0  0.083333   \n",
       "2    968-81-0    1989  324.114378  2.960  100.72  0.0  0.0  0.0  0.185395   \n",
       "3    520-45-6  122903  168.042259 -0.551   60.44  0.0  0.0  0.0  0.055556   \n",
       "4  50594-66-6   44073  360.996485  4.557   89.67  0.0  0.0  0.0  0.136083   \n",
       "\n",
       "        V10  ...  V1059  V1060  V1061  V1064  V1070  V1071  V1072  V1073  \\\n",
       "0  0.184195  ...      0      0      0      0      0      0      0      0   \n",
       "1  0.142259  ...      0      0      0      0      0      0      0      0   \n",
       "2  0.161948  ...      0      0      0      0      0      0      0      0   \n",
       "3  0.198742  ...      0      0      0      0      0      0      0      0   \n",
       "4  0.276855  ...      0      0      0      0      0      0      0      0   \n",
       "\n",
       "   V1074  V1075  \n",
       "0      0      0  \n",
       "1      0      0  \n",
       "2      0      0  \n",
       "3      0      0  \n",
       "4      0      0  \n",
       "\n",
       "[5 rows x 886 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data.rename(columns = {'V1':'chemical_id'}, inplace = True) \n",
    "train_data = train_data.merge(feature_data, on=\"chemical_id\", how=\"left\")\n",
    "train_data.drop(['chemical_id', 'Id','V2', 'Expected'], axis = 1, inplace = True)\n",
    "test_data = test_data.merge(feature_data, on=\"chemical_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  assay_id          V3     V4      V5   V6   V7        V8        V9       V10  \\\n",
      "0     1682  164.120115  3.659   20.23  0.0  0.0  0.000000  0.068041  0.170103   \n",
      "1     1656  431.056940  1.258  183.30  0.0  0.0  0.000000  0.151375  0.227671   \n",
      "2       36  695.250845  6.365   95.92  0.0  0.0  0.174792  0.789110  1.360933   \n",
      "3     1850  200.949810 -1.340   68.82  0.0  0.0  0.000000  0.058926  0.235702   \n",
      "4       30  418.271924  4.775   72.83  0.0  0.0  0.000000  0.179152  0.516591   \n",
      "\n",
      "   V11  ...  V1059  V1060  V1061  V1064  V1070  V1071  V1072  V1073  V1074  \\\n",
      "0  0.0  ...      0      0      0      0      0      0      0      0      0   \n",
      "1  0.0  ...      0      0      0      0      0      0      0      0      0   \n",
      "2  0.0  ...      0      0      0      0      0      0      0      0      0   \n",
      "3  0.0  ...      0      0      0      0      0      0      0      0      0   \n",
      "4  0.0  ...      0      0      0      0      0      0      0      0      0   \n",
      "\n",
      "   V1075  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "\n",
      "[5 rows x 885 columns]\n",
      "Index(['assay_id', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
      "       ...\n",
      "       'V1059', 'V1060', 'V1061', 'V1064', 'V1070', 'V1071', 'V1072', 'V1073',\n",
      "       'V1074', 'V1075'],\n",
      "      dtype='object', length=885)\n"
     ]
    }
   ],
   "source": [
    "id_list = test_data.x\n",
    "test_data = test_data.drop(['chemical_id','x', 'V2'], axis = 1, inplace = False)\n",
    "print(test_data.head())\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train_data = min_max_scaler.fit_transform(train_data)\n",
    "test_data = min_max_scaler.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruthvikjaini\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:06:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:11:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:16:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:21:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:27:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:33:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:39:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:45:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:52:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:00:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=None, shuffle=False)\n",
    "xgb_acc_score = []\n",
    "xgb = XGBClassifier(n_estimators=200, max_depth=6)\n",
    "X = train_data\n",
    "y = traget_data\n",
    "for train_index , test_index in kf.split(X, y):\n",
    "    X_train , X_test = X[train_index,:],X[test_index,:]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "    xgb.fit(X_train,y_train)\n",
    "    xgb_pred_values = xgb.predict(X_test)\n",
    "    xgb_acc = accuracy_score(xgb_pred_values , y_test)\n",
    "    xgb_acc_score.append(xgb_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy of XGBClassifier : 0.9085812272022566\n",
      "f1_score of xgb(SKfold) : 0.7831059001343305\n"
     ]
    }
   ],
   "source": [
    "avg_acc_score = sum(xgb_acc_score)/10\n",
    "print('Avg accuracy of XGBClassifier : {}'.format(avg_acc_score))\n",
    "f1 = f1_score(xgb_pred_values , y_test, average = 'macro')\n",
    "print('f1_score of xgb(SKfold) : {}'.format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = xgb.predict(test_data)\n",
    "df_submission = pd.DataFrame({'Id':  id_list, 'Predicted': prediction})\n",
    "df_submission.to_csv('xgb_Submission(SKfold_-1000col).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
